# ğŸŒŠ HTTP Streaming Implementation Guide

## Overview

HÆ°á»›ng dáº«n chi tiáº¿t vá» viá»‡c implement **HTTP Streaming** (Server-Sent Events) Ä‘á»ƒ stream AI responses real-time nhÆ° ChatGPT!

## âœ¨ Features

### 1. **Real-time Streaming**
- Text xuáº¥t hiá»‡n tá»« tá»« word-by-word
- Giá»‘ng há»‡t ChatGPT experience
- Smooth vÃ  professional

### 2. **Regenerate Function**
- Click Ä‘á»ƒ generate láº¡i response
- Keep history cá»§a cÃ¡c responses
- Confidence score cho má»—i response

### 3. **Confidence Score**
- Display reliability percentage (95%, 88%, etc.)
- Color-coded: Green (>90%), Yellow (70-90%), Red (<70%)
- Visual feedback cho user

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP POST       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>   â”‚  API Route  â”‚
â”‚  (Browser)  â”‚                      â”‚   (Next.js) â”‚
â”‚             â”‚  <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚             â”‚
â”‚             â”‚  SSE Stream (text)   â”‚             â”‚
â”‚             â”‚  SSE Stream (conf)   â”‚             â”‚
â”‚             â”‚  SSE Stream (meta)   â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      Close stream    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Components

### 1. **useAIStream Hook**
Location: `/src/core/hooks/use-ai-stream.ts`

Custom hook Ä‘á»ƒ handle streaming:

```tsx
const { streamAI, isStreaming, streamedText, error, abort, reset } = useAIStream({
  onChunk: (chunk) => console.log("Chunk:", chunk),
  onComplete: (fullText) => console.log("Done:", fullText),
  onError: (error) => console.error("Error:", error),
});

// Use it
const result = await streamAI("/api/ai/refine", { issueId: "123" });
console.log(result.text, result.confidence);
```

**Methods:**
- `streamAI(url, payload)` - Start streaming
- `abort()` - Cancel current stream
- `reset()` - Reset state

**State:**
- `isStreaming` - Is currently streaming?
- `streamedText` - Real-time accumulated text
- `error` - Error if any

### 2. **AIResponseCard Component**
Location: `/src/core/components/issue/ai-response-card.tsx`

Card hiá»ƒn thá»‹ AI response vá»›i regenerate button:

```tsx
<AIResponseCard
  title="AI Refined Description"
  content={responseText}
  confidence={95}
  onRegenerate={() => regenerate()}
  isRegenerating={false}
/>
```

**Props:**
- `title` - Card title
- `content` - AI generated content
- `confidence` - Score 0-100
- `onRegenerate` - Callback khi click regenerate
- `isRegenerating` - Loading state

### 3. **AIActionButtonsV2 Component**
Location: `/src/core/components/issue/ai-action-buttons-v2.tsx`

Main component vá»›i streaming support:

```tsx
<AIActionButtonsV2
  issueId="123"
  onRefine={(url, payload) => ({ url, payload })}
  onEstimate={(url, payload) => ({ url, payload })}
  onBreakdown={(url, payload) => ({ url, payload })}
/>
```

## ğŸ”§ API Implementation

### Server-Side (Next.js API Route)

File: `/src/app/api/ai/refine/route.ts`

```typescript
export async function POST(request: NextRequest) {
  const { issueId } = await request.json();

  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      try {
        const aiResponse = "Your AI generated text...";
        const words = aiResponse.split(" ");

        // Stream word by word
        for (const word of words) {
          const data = { type: "text", content: word + " " };
          controller.enqueue(
            encoder.encode(`data: ${JSON.stringify(data)}\n\n`)
          );
          await new Promise((resolve) => setTimeout(resolve, 50));
        }

        // Send confidence
        const confidenceData = { type: "confidence", value: 95 };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(confidenceData)}\n\n`)
        );

        // Send metadata
        const metadataData = {
          type: "metadata",
          value: { model: "gpt-4", tokens: 100 },
        };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(metadataData)}\n\n`)
        );

        controller.close();
      } catch (error) {
        const errorData = {
          type: "error",
          message: error.message,
        };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(errorData)}\n\n`)
        );
        controller.close();
      }
    },
  });

  return new Response(stream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}
```

### Message Types

#### 1. Text Chunk
```json
{
  "type": "text",
  "content": "word "
}
```

#### 2. Confidence Score
```json
{
  "type": "confidence",
  "value": 95
}
```

#### 3. Metadata
```json
{
  "type": "metadata",
  "value": {
    "model": "gpt-4",
    "tokens": 100,
    "timestamp": "2025-01-15T10:00:00Z"
  }
}
```

#### 4. Error
```json
{
  "type": "error",
  "message": "Error description"
}
```

## ğŸš€ Usage Example

### Full Implementation

```tsx
import { AIActionButtonsV2 } from "@/core/components/issue/ai-action-buttons-v2";

export const IssueDetailPanel = ({ issueId }: { issueId: string }) => {
  return (
    <div className="space-y-4">
      <AIActionButtonsV2
        issueId={issueId}
        onRefine={(url, payload) => ({
          url: "/api/ai/refine",
          payload: { ...payload, issueId },
        })}
        onEstimate={(url, payload) => ({
          url: "/api/ai/estimate",
          payload: { ...payload, issueId },
        })}
        onBreakdown={(url, payload) => ({
          url: "/api/ai/breakdown",
          payload: { ...payload, issueId },
        })}
      />
    </div>
  );
};
```

## ğŸ’¡ Integration vá»›i Real AI Service

### OpenAI Example

```typescript
import OpenAI from "openai";

export async function POST(request: NextRequest) {
  const { issueId, description } = await request.json();
  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      try {
        const completion = await openai.chat.completions.create({
          model: "gpt-4",
          messages: [
            {
              role: "system",
              content: "You are a helpful assistant that refines issue descriptions.",
            },
            {
              role: "user",
              content: `Please refine this issue description: ${description}`,
            },
          ],
          stream: true,
        });

        for await (const chunk of completion) {
          const content = chunk.choices[0]?.delta?.content;
          if (content) {
            const data = { type: "text", content };
            controller.enqueue(
              encoder.encode(`data: ${JSON.stringify(data)}\n\n`)
            );
          }
        }

        const confidenceData = { type: "confidence", value: 95 };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(confidenceData)}\n\n`)
        );

        controller.close();
      } catch (error) {
        const errorData = {
          type: "error",
          message: error.message,
        };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(errorData)}\n\n`)
        );
        controller.close();
      }
    },
  });

  return new Response(stream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}
```

### Anthropic Claude Example

```typescript
import Anthropic from "@anthropic-ai/sdk";

export async function POST(request: NextRequest) {
  const { issueId, description } = await request.json();
  const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      try {
        const completion = await anthropic.messages.stream({
          model: "claude-3-5-sonnet-20241022",
          max_tokens: 1024,
          messages: [
            {
              role: "user",
              content: `Refine this issue description: ${description}`,
            },
          ],
        });

        for await (const chunk of completion) {
          if (
            chunk.type === "content_block_delta" &&
            chunk.delta.type === "text_delta"
          ) {
            const data = { type: "text", content: chunk.delta.text };
            controller.enqueue(
              encoder.encode(`data: ${JSON.stringify(data)}\n\n`)
            );
          }
        }

        controller.close();
      } catch (error) {
        const errorData = { type: "error", message: error.message };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(errorData)}\n\n`)
        );
        controller.close();
      }
    },
  });

  return new Response(stream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}
```

## ğŸ¯ Best Practices

### 1. **Error Handling**
```typescript
try {
  const result = await streamAI(url, payload);
} catch (error) {
  console.error("Streaming error:", error);
  // Show error toast to user
}
```

### 2. **Abort Long-Running Streams**
```typescript
const { streamAI, abort } = useAIStream();

// Start streaming
streamAI(url, payload);

// User clicks cancel
onCancel(() => abort());
```

### 3. **Rate Limiting**
Implement rate limiting trÃªn server:

```typescript
import { ratelimit } from "@/lib/ratelimit";

export async function POST(request: NextRequest) {
  const ip = request.headers.get("x-forwarded-for") ?? "unknown";
  const { success } = await ratelimit.limit(ip);

  if (!success) {
    return new Response("Too many requests", { status: 429 });
  }

  // Continue with streaming...
}
```

### 4. **Caching Responses**
Cache AI responses Ä‘á»ƒ trÃ¡nh regenerate tá»‘n tiá»n:

```typescript
import { redis } from "@/lib/redis";

const cacheKey = `ai:refine:${issueId}`;
const cached = await redis.get(cacheKey);

if (cached) {
  return new Response(cached, {
    headers: { "Content-Type": "application/json" },
  });
}

// Generate and cache...
await redis.set(cacheKey, result, { ex: 3600 }); // 1 hour
```

## ğŸ“Š Performance

### Metrics

- **Streaming Speed**: 50ms/word (configurable)
- **First Byte**: < 100ms
- **Total Time**: Depends on content length
- **Memory Usage**: Minimal (streaming mode)

### Optimization

1. **Adjust word delay** based on network speed
2. **Batch small words** Ä‘á»ƒ reduce overhead
3. **Use compression** for large responses
4. **Implement connection pooling** cho multiple requests

## ğŸ› Troubleshooting

### Stream khÃ´ng hoáº¡t Ä‘á»™ng?
- Check network tab trong DevTools
- Verify headers: `Content-Type: text/event-stream`
- Check CORS settings

### Text bá»‹ delay?
- Giáº£m delay giá»¯a cÃ¡c words (hiá»‡n táº¡i: 50ms)
- Check server response time

### Memory leak?
- Äáº£m báº£o `controller.close()` Ä‘Æ°á»£c gá»i
- Cleanup AbortController Ä‘Ãºng cÃ¡ch

## ğŸ“š References

- [MDN: Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- [Next.js: Streaming](https://nextjs.org/docs/app/building-your-application/routing/route-handlers#streaming)
- [Web Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API)

## ğŸ‰ Result

Vá»›i implementation nÃ y, báº¡n sáº½ cÃ³:
- âœ… Real-time streaming nhÆ° ChatGPT
- âœ… Regenerate functionality
- âœ… Confidence scores
- âœ… Error handling
- âœ… Professional UX
- âœ… Easy integration vá»›i OpenAI/Claude/etc.

**Users will be amazed! ğŸ¤©**
